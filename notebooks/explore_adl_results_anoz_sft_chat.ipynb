{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ADL Results Explorer — first_letter_anoz SFT Chat\n",
    "\n",
    "Explores Logit Lens and PatchScope outputs from the Activation Difference Lens pipeline.\n",
    "Run on chat-formatted data (`tulu-3-sft-olmo-2-mixture`) with the WizardLM SFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration (edit these) ---\n",
    "RESULTS_DIR = Path(\"/workspace/model-organisms/diffing_results/olmo2_1B/first_letter_anoz_sft_chat/activation_difference_lens\")\n",
    "LAYERS = [14, 15]\n",
    "DATASET = \"tulu-3-sft-olmo-2-mixture\"\n",
    "PRE_ASSISTANT_K = 3          # Positions before assistant turn (negative labels: -k..-1)\n",
    "N = 128                      # Positions after assistant turn start (config: n)\n",
    "POSITIONS = list(range(-PRE_ASSISTANT_K, N))  # All position labels: [-3, -2, -1, 0, ..., 127]\n",
    "LOGIT_LENS_POSITION = 0      # Position label for per-position logit lens view\n",
    "PATCHSCOPE_POSITION = 0      # Position label for per-position patchscope view\n",
    "LOGIT_LENS_MAX_ROWS = None   # Set to an integer to truncate logit lens tables\n",
    "PATCHSCOPE_GRADER = \"openai_gpt-5-mini\"\n",
    "MODEL_ID = \"/workspace/models/olmo2_1b_base\"\n",
    "\n",
    "LAYER_DIRS = {layer: RESULTS_DIR / f\"layer_{layer}\" / DATASET for layer in LAYERS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", 60)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "\n",
    "def fmt_prob(p):\n",
    "    \"\"\"Format probability: scientific notation for small values, fixed for larger.\"\"\"\n",
    "    if abs(p) < 0.01:\n",
    "        return f\"{p:.2e}\"\n",
    "    return f\"{p:.4f}\"\n",
    "\n",
    "\n",
    "def display_token(t):\n",
    "    \"\"\"Make whitespace-only or invisible tokens visible via repr.\"\"\"\n",
    "    if not t.strip():\n",
    "        return repr(t)\n",
    "    return t\n",
    "\n",
    "\n",
    "def _normalize_token(t):\n",
    "    \"\"\"Strip tokenizer space markers (sentencepiece, GPT-2) for comparison.\"\"\"\n",
    "    return t.replace(\"\\u2581\", \"\").replace(\"\\u0120\", \"\").strip()\n",
    "\n",
    "\n",
    "def load_logit_lens(layer, pos, prefix=\"\"):\n",
    "    \"\"\"Load logit lens .pt file. Returns (top_k_probs, top_k_indices, inv_probs, inv_indices).\"\"\"\n",
    "    return torch.load(LAYER_DIRS[layer] / f\"{prefix}logit_lens_pos_{pos}.pt\", weights_only=True)\n",
    "\n",
    "\n",
    "def decode_tokens(indices):\n",
    "    return [tokenizer.decode([int(i)]) for i in indices]\n",
    "\n",
    "\n",
    "def load_patchscope(layer, pos, prefix=\"\"):\n",
    "    \"\"\"Load auto_patch_scope .pt file. Returns dict with tokens_at_best_scale, selected_tokens, etc.\"\"\"\n",
    "    return torch.load(\n",
    "        LAYER_DIRS[layer] / f\"{prefix}auto_patch_scope_pos_{pos}_{PATCHSCOPE_GRADER}.pt\",\n",
    "        weights_only=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def discover_patchscope_positions(layer):\n",
    "    \"\"\"Find which positions have patchscope results (diff variant). Handles negative labels.\"\"\"\n",
    "    positions = []\n",
    "    for f in sorted(LAYER_DIRS[layer].glob(f\"auto_patch_scope_pos_*_{PATCHSCOPE_GRADER}.pt\")):\n",
    "        m = re.search(r\"auto_patch_scope_pos_(-?\\d+)_\", f.name)\n",
    "        if m:\n",
    "            positions.append(int(m.group(1)))\n",
    "    return sorted(positions)\n",
    "\n",
    "\n",
    "def concat_layer_dfs(dfs):\n",
    "    \"\"\"Pad DataFrames to equal length with empty strings, then concatenate horizontally.\"\"\"\n",
    "    max_len = max(len(df) for df in dfs)\n",
    "    padded = []\n",
    "    for df in dfs:\n",
    "        if len(df) < max_len:\n",
    "            pad = pd.DataFrame(\n",
    "                {col: [\"\"] * (max_len - len(df)) for col in df.columns},\n",
    "                index=range(len(df), max_len),\n",
    "            )\n",
    "            df = pd.concat([df, pad], axis=0)\n",
    "        padded.append(df)\n",
    "    return pd.concat(padded, axis=1)\n",
    "\n",
    "\n",
    "for layer in LAYERS:\n",
    "    print(f\"Layer {layer} dir: {LAYER_DIRS[layer]}\")\n",
    "    print(f\"  PatchScope positions: {discover_patchscope_positions(layer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Logit Lens Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### 1A. Single Position\n",
    "\n",
    "Each column shows the top-100 (or bottom-100 for `_inv`) tokens from the logit lens projection.  \n",
    "Negative positions are before the assistant turn; position 0 is the first assistant token.  \n",
    "Format: `token (softmax_prob)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit lens columns: (file prefix, tuple index for probs, tuple index for indices)\n",
    "LL_VARIANTS = {\n",
    "    \"base\":     (\"base_\", 0, 1),\n",
    "    \"base_inv\": (\"base_\", 2, 3),\n",
    "    \"ft\":       (\"ft_\",   0, 1),\n",
    "    \"ft_inv\":   (\"ft_\",   2, 3),\n",
    "    \"diff\":     (\"\",      0, 1),\n",
    "    \"diff_inv\": (\"\",      2, 3),\n",
    "}\n",
    "\n",
    "\n",
    "def logit_lens_position_table_single(layer, pos):\n",
    "    cols = {}\n",
    "    for col_name, (prefix, pi, ii) in LL_VARIANTS.items():\n",
    "        data = load_logit_lens(layer, pos, prefix)\n",
    "        tokens = decode_tokens(data[ii])\n",
    "        probs = data[pi].tolist()\n",
    "        cols[col_name] = [f\"{display_token(t)} ({fmt_prob(p)})\" for t, p in zip(tokens, probs)]\n",
    "    df = pd.DataFrame(cols)\n",
    "    if LOGIT_LENS_MAX_ROWS is not None:\n",
    "        df = df.head(LOGIT_LENS_MAX_ROWS)\n",
    "    return df\n",
    "\n",
    "\n",
    "def logit_lens_position_table(pos):\n",
    "    dfs = []\n",
    "    for layer in LAYERS:\n",
    "        df = logit_lens_position_table_single(layer, pos)\n",
    "        df.columns = pd.MultiIndex.from_product([[f\"layer_{layer}\"], df.columns])\n",
    "        dfs.append(df)\n",
    "    return concat_layer_dfs(dfs)\n",
    "\n",
    "\n",
    "print(f\"Logit lens at position {LOGIT_LENS_POSITION}:\")\n",
    "logit_lens_position_table(LOGIT_LENS_POSITION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### 1B. Aggregated Across All Positions\n",
    "\n",
    "For each column, tokens are ranked by their average probability across all positions (tokens not in the top/bottom 100 for a given position contribute p=0).  \n",
    "Format: `token (avg_prob)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens_aggregated_single(layer):\n",
    "    agg = {}\n",
    "    for col_name, (prefix, pi, ii) in LL_VARIANTS.items():\n",
    "        token_prob_sum = defaultdict(float)\n",
    "        for pos in POSITIONS:\n",
    "            data = load_logit_lens(layer, pos, prefix)\n",
    "            tokens = decode_tokens(data[ii])\n",
    "            probs = data[pi].tolist()\n",
    "            for t, p in zip(tokens, probs):\n",
    "                token_prob_sum[t] += p\n",
    "        token_avg = {t: s / len(POSITIONS) for t, s in token_prob_sum.items()}\n",
    "        sorted_tokens = sorted(token_avg, key=lambda t: (-token_avg[t], t))\n",
    "        limit = LOGIT_LENS_MAX_ROWS if LOGIT_LENS_MAX_ROWS is not None else 100\n",
    "        agg[col_name] = [\n",
    "            f\"{display_token(t)} ({fmt_prob(token_avg[t])})\" for t in sorted_tokens[:limit]\n",
    "        ]\n",
    "\n",
    "    max_len = max(len(v) for v in agg.values())\n",
    "    for k in agg:\n",
    "        agg[k] += [\"\"] * (max_len - len(agg[k]))\n",
    "    return pd.DataFrame(agg)\n",
    "\n",
    "\n",
    "def logit_lens_aggregated():\n",
    "    dfs = []\n",
    "    for layer in LAYERS:\n",
    "        df = logit_lens_aggregated_single(layer)\n",
    "        df.columns = pd.MultiIndex.from_product([[f\"layer_{layer}\"], df.columns])\n",
    "        dfs.append(df)\n",
    "    return concat_layer_dfs(dfs)\n",
    "\n",
    "\n",
    "print(\"Logit lens aggregated across all positions:\")\n",
    "logit_lens_aggregated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 2. PatchScope Analysis\n",
    "\n",
    "PatchScope injects the activation vector into the model at varying scales and decodes the output.  \n",
    "Unlike logit lens, there are no inverse variants -- only `base`, `ft`, and `diff`.  \n",
    "Tokens marked with a green checkmark were selected by the LLM grader as semantically coherent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 2A. Single Position\n",
    "\n",
    "Shows tokens at the best scale found by the auto patch scope search.  \n",
    "Format: `token (prob)` with `✅` if in `selected_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS_VARIANTS = [(\"base\", \"base_\"), (\"ft\", \"ft_\"), (\"diff\", \"\")]\n",
    "\n",
    "\n",
    "def patchscope_position_table_single(layer, pos):\n",
    "    cols = {}\n",
    "    for col_name, prefix in PS_VARIANTS:\n",
    "        data = load_patchscope(layer, pos, prefix)\n",
    "        tokens = data[\"tokens_at_best_scale\"]\n",
    "        selected = {_normalize_token(t) for t in data[\"selected_tokens\"]}\n",
    "        probs = data[\"token_probs\"]\n",
    "        cols[col_name] = [\n",
    "            f\"{display_token(t)} ({fmt_prob(p)})\" + (\" \\u2705\" if _normalize_token(t) in selected else \"\")\n",
    "            for t, p in zip(tokens, probs)\n",
    "        ]\n",
    "\n",
    "    max_len = max(len(v) for v in cols.values())\n",
    "    for k in cols:\n",
    "        cols[k] += [\"\"] * (max_len - len(cols[k]))\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "\n",
    "def patchscope_position_table(pos):\n",
    "    dfs = []\n",
    "    for layer in LAYERS:\n",
    "        df = patchscope_position_table_single(layer, pos)\n",
    "        df.columns = pd.MultiIndex.from_product([[f\"layer_{layer}\"], df.columns])\n",
    "        dfs.append(df)\n",
    "    return concat_layer_dfs(dfs)\n",
    "\n",
    "\n",
    "print(f\"PatchScope at position {PATCHSCOPE_POSITION}:\")\n",
    "patchscope_position_table(PATCHSCOPE_POSITION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 2B. Aggregated Across All PatchScope Positions\n",
    "\n",
    "Tokens ranked by average probability across all patchscope positions (p=0 if absent for a given position).  \n",
    "Green checkmark if the token was in `selected_tokens` for **any** position.  \n",
    "Format: `token (avg_prob)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchscope_aggregated_single(layer):\n",
    "    ps_positions = discover_patchscope_positions(layer)\n",
    "    n_ps = len(ps_positions)\n",
    "\n",
    "    cols = {}\n",
    "    for col_name, prefix in PS_VARIANTS:\n",
    "        token_prob_sum = defaultdict(float)\n",
    "        ever_selected = set()\n",
    "        for pos in ps_positions:\n",
    "            data = load_patchscope(layer, pos, prefix)\n",
    "            tokens = data[\"tokens_at_best_scale\"]\n",
    "            probs = data[\"token_probs\"]\n",
    "            for t, p in zip(tokens, probs):\n",
    "                token_prob_sum[t] += p\n",
    "            ever_selected.update(_normalize_token(t) for t in data[\"selected_tokens\"])\n",
    "\n",
    "        token_avg = {t: s / n_ps for t, s in token_prob_sum.items()}\n",
    "        sorted_tokens = sorted(token_avg, key=lambda t: (-token_avg[t], t))\n",
    "        cols[col_name] = [\n",
    "            f\"{display_token(t)} ({fmt_prob(token_avg[t])})\" + (\" \\u2705\" if _normalize_token(t) in ever_selected else \"\")\n",
    "            for t in sorted_tokens\n",
    "        ]\n",
    "\n",
    "    max_len = max(len(v) for v in cols.values())\n",
    "    for k in cols:\n",
    "        cols[k] += [\"\"] * (max_len - len(cols[k]))\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "\n",
    "def patchscope_aggregated():\n",
    "    dfs = []\n",
    "    for layer in LAYERS:\n",
    "        df = patchscope_aggregated_single(layer)\n",
    "        df.columns = pd.MultiIndex.from_product([[f\"layer_{layer}\"], df.columns])\n",
    "        dfs.append(df)\n",
    "    return concat_layer_dfs(dfs)\n",
    "\n",
    "\n",
    "ps_pos_str = {layer: discover_patchscope_positions(layer) for layer in LAYERS}\n",
    "print(f\"PatchScope aggregated across positions: {ps_pos_str}\")\n",
    "patchscope_aggregated()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
