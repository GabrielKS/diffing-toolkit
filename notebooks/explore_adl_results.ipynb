{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d608a54",
   "metadata": {},
   "source": [
    "# Some basic ADL results exploration\n",
    "generated by Claude Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595de69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results directory: /workspace/model-organisms/diffing_results/gemma3_1B/cake_bake/activation_difference_lens copy/layer_12/fineweb-1m-sample\n",
      "‚úÖ Directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set paths\n",
    "results_dir = Path(\"/workspace/model-organisms/diffing_results/gemma3_1B/cake_bake/activation_difference_lens copy/layer_12/fineweb-1m-sample\")\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f\"‚úÖ Results directory: {results_dir}\")\n",
    "print(f\"‚úÖ Directory exists: {results_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fe9a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AVAILABLE RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä Logit Lens Results:\n",
      "   ‚Ä¢ Difference vectors: 128 positions\n",
      "   ‚Ä¢ Base model: 128 positions\n",
      "   ‚Ä¢ Finetuned model: 128 positions\n",
      "   ‚Ä¢ Position range: 0 to 127\n",
      "\n",
      "üìä Auto Patchscope Results:\n",
      "   ‚Ä¢ Difference interpretations: 6 positions\n",
      "   ‚Ä¢ Base model: 6 positions\n",
      "   ‚Ä¢ Finetuned model: 6 positions\n",
      "   ‚Ä¢ Available positions: [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Summary of available results\n",
    "\n",
    "# Count files\n",
    "logit_lens_files = list(results_dir.glob(\"logit_lens_pos_*.pt\"))\n",
    "base_logit_lens_files = list(results_dir.glob(\"base_logit_lens_pos_*.pt\"))\n",
    "ft_logit_lens_files = list(results_dir.glob(\"ft_logit_lens_pos_*.pt\"))\n",
    "patchscope_files = list(results_dir.glob(\"auto_patch_scope_pos_*_openai_gpt-5-mini.pt\"))\n",
    "base_patchscope_files = list(results_dir.glob(\"base_auto_patch_scope_pos_*.pt\"))\n",
    "ft_patchscope_files = list(results_dir.glob(\"ft_auto_patch_scope_pos_*.pt\"))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AVAILABLE RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Logit Lens Results:\")\n",
    "print(f\"   ‚Ä¢ Difference vectors: {len(logit_lens_files)} positions\")\n",
    "print(f\"   ‚Ä¢ Base model: {len(base_logit_lens_files)} positions\")\n",
    "print(f\"   ‚Ä¢ Finetuned model: {len(ft_logit_lens_files)} positions\")\n",
    "print(f\"   ‚Ä¢ Position range: 0 to {len(logit_lens_files) - 1}\")\n",
    "\n",
    "print(f\"\\nüìä Auto Patchscope Results:\")\n",
    "print(f\"   ‚Ä¢ Difference interpretations: {len(patchscope_files)} positions\")\n",
    "print(f\"   ‚Ä¢ Base model: {len(base_patchscope_files)} positions\")\n",
    "print(f\"   ‚Ä¢ Finetuned model: {len(ft_patchscope_files)} positions\")\n",
    "\n",
    "# Extract patchscope positions\n",
    "# Filename format: auto_patch_scope_pos_0_openai_gpt-5-mini.pt\n",
    "# Split by '_' gives: ['auto', 'patch', 'scope', 'pos', '0', 'openai', 'gpt', '5', 'mini.pt']\n",
    "# Position number is at index 4\n",
    "patchscope_positions = sorted([\n",
    "    int(f.name.split(\"_\")[4]) \n",
    "    for f in patchscope_files\n",
    "])\n",
    "print(f\"   ‚Ä¢ Available positions: {patchscope_positions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e91f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3xlqw1hlxzw",
   "metadata": {},
   "source": [
    "## Logit Lens Analysis\n",
    "\n",
    "The logit lens projects activation differences through the unembedding layer to see which tokens they predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nkpph7ahu3q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Logit Lens Structure (Position 63):\n",
      "   ‚Ä¢ top_k_probs: shape=torch.Size([100]) (probabilities of tokens that INCREASE)\n",
      "   ‚Ä¢ top_k_indices: shape=torch.Size([100]) (token IDs that INCREASE)\n",
      "   ‚Ä¢ top_k_inv_probs: shape=torch.Size([100]) (probabilities of tokens that DECREASE)\n",
      "   ‚Ä¢ top_k_inv_indices: shape=torch.Size([100]) (token IDs that DECREASE)\n",
      "\n",
      "   Note: 100 tokens cached per position (top 100 increases and top 100 decreases)\n",
      "\n",
      "üîº Top 10 tokens that INCREASE in probability:\n",
      "    1. íÜù                    (ID: 252977, prob: 0.006744)\n",
      "    2. HtIdx                (ID:  61262, prob: 0.004089)\n",
      "    3. íåæ                    (ID: 253101, prob: 0.002472)\n",
      "    4. Íóï                    (ID: 250244, prob: 0.002472)\n",
      "    5. Polynucleaires       (ID: 193802, prob: 0.002472)\n",
      "    6. íÇÄ                    (ID: 250668, prob: 0.002472)\n",
      "    7. Ê∏¶Êü±                   (ID: 204369, prob: 0.002472)\n",
      "    8. íÜ£                    (ID: 250517, prob: 0.001503)\n",
      "    9. ÍóÆ                    (ID: 250268, prob: 0.001503)\n",
      "   10. íÅ£                    (ID: 252901, prob: 0.001503)\n",
      "\n",
      "üîΩ Top 10 tokens that DECREASE in probability:\n",
      "    1. \n",
      "                    (ID:    107, prob: 0.808594)\n",
      "    2.  (                   (ID:    568, prob: 0.109375)\n",
      "    3.  and                 (ID:    532, prob: 0.066406)\n",
      "    4.                      (ID: 236743, prob: 0.014771)\n",
      "    5. ...                  (ID:   1390, prob: 0.001213)\n",
      "    6.  –∏                   (ID:   1079, prob: 0.000648)\n",
      "    7.                      (ID:    138, prob: 0.000078)\n",
      "    8.  i                   (ID:    858, prob: 0.000017)\n",
      "    9. /                    (ID: 236786, prob: 0.000015)\n",
      "   10. -                    (ID: 236772, prob: 0.000007)\n"
     ]
    }
   ],
   "source": [
    "# Load logit lens results for position 0\n",
    "position = 63\n",
    "\n",
    "# Difference logit lens (finetuned - base)\n",
    "ll_diff_path = results_dir / f\"logit_lens_pos_{position}.pt\"\n",
    "top_k_probs, top_k_indices, top_k_inv_probs, top_k_inv_indices = torch.load(ll_diff_path, map_location=\"cpu\")\n",
    "\n",
    "print(f\"üìä Logit Lens Structure (Position {position}):\")\n",
    "print(f\"   ‚Ä¢ top_k_probs: shape={top_k_probs.shape} (probabilities of tokens that INCREASE)\")\n",
    "print(f\"   ‚Ä¢ top_k_indices: shape={top_k_indices.shape} (token IDs that INCREASE)\")\n",
    "print(f\"   ‚Ä¢ top_k_inv_probs: shape={top_k_inv_probs.shape} (probabilities of tokens that DECREASE)\")\n",
    "print(f\"   ‚Ä¢ top_k_inv_indices: shape={top_k_inv_indices.shape} (token IDs that DECREASE)\")\n",
    "print(f\"\\n   Note: 100 tokens cached per position (top 100 increases and top 100 decreases)\")\n",
    "\n",
    "# Decode top tokens\n",
    "print(f\"\\nüîº Top 10 tokens that INCREASE in probability:\")\n",
    "for i in range(10):\n",
    "    token_id = top_k_indices[i].item()\n",
    "    prob = top_k_probs[i].item()\n",
    "    token_str = tokenizer.decode([token_id])\n",
    "    print(f\"   {i+1:2d}. {token_str:20s} (ID: {token_id:6d}, prob: {prob:.6f})\")\n",
    "\n",
    "print(f\"\\nüîΩ Top 10 tokens that DECREASE in probability:\")\n",
    "for i in range(10):\n",
    "    token_id = top_k_inv_indices[i].item()\n",
    "    prob = top_k_inv_probs[i].item()\n",
    "    token_str = tokenizer.decode([token_id])\n",
    "    print(f\"   {i+1:2d}. {token_str:20s} (ID: {token_id:6d}, prob: {prob:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9vdml7f2eh",
   "metadata": {},
   "source": [
    "## Auto Patchscope Analysis\n",
    "\n",
    "Auto Patchscope injects activation differences into various prompts to interpret their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fyunhpm5gn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Patchscope Structure (Position 5):\n",
      "   Keys: ['best_scale', 'tokens_at_best_scale', 'selected_tokens', 'token_probs', 'normalized']\n",
      "\n",
      "   ‚Ä¢ best_scale: float = 20.0\n",
      "   ‚Ä¢ tokens_at_best_scale: list length=20\n",
      "      First item type: <class 'str'>\n",
      "      First 3 items: [' ÎãπÏã†', ' masterful', ' unrival']\n",
      "   ‚Ä¢ selected_tokens: list length=16\n",
      "      First item type: <class 'str'>\n",
      "      First 3 items: ['masterful', 'unrival', 'groundbreaking']\n",
      "   ‚Ä¢ token_probs: list length=20\n",
      "      First item type: <class 'float'>\n",
      "   ‚Ä¢ normalized: bool = True\n"
     ]
    }
   ],
   "source": [
    "# Load Patchscope results for position 0\n",
    "position = 5\n",
    "\n",
    "aps_path = results_dir / f\"auto_patch_scope_pos_{position}_openai_gpt-5-mini.pt\"\n",
    "aps_data = torch.load(aps_path, map_location=\"cpu\")\n",
    "\n",
    "print(f\"üìä Patchscope Structure (Position {position}):\")\n",
    "print(f\"   Keys: {list(aps_data.keys())}\")\n",
    "print()\n",
    "\n",
    "# Show details of each key\n",
    "for key, value in aps_data.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"   ‚Ä¢ {key}: Tensor shape={value.shape}\")\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        print(f\"   ‚Ä¢ {key}: {type(value).__name__} length={len(value)}\")\n",
    "        if len(value) > 0:\n",
    "            print(f\"      First item type: {type(value[0])}\")\n",
    "            if isinstance(value[0], str):\n",
    "                print(f\"      First 3 items: {value[:3]}\")\n",
    "    elif isinstance(value, dict):\n",
    "        print(f\"   ‚Ä¢ {key}: dict with {len(value)} keys\")\n",
    "        print(f\"      Keys: {list(value.keys())[:5]}...\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ {key}: {type(value).__name__} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7zj7hi1fcug",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top Patchscope Tokens (Position 5):\n",
      "   These are the tokens selected as most relevant by Patchscope analysis\n",
      "\n",
      "   Number of tokens: 20\n",
      "   Top 20 tokens:\n",
      "       1. ' ÎãπÏã†'\n",
      "       2. ' masterful'\n",
      "       3. ' unrival'\n",
      "       4. ' groundbreaking'\n",
      "       5. ' visionary'\n",
      "       6. ' transcendent'\n",
      "       7. ' ÏòàÏà†'\n",
      "       8. ' roadway'\n",
      "       9. ' exquis'\n",
      "      10. ' sidewalk'\n",
      "      11. ' meditative'\n",
      "      12. ' Í≥ºÌïô'\n",
      "      13. ' ÏµúÍ≥†Ïùò'\n",
      "      14. ' unparalleled'\n",
      "      15. ' momentous'\n",
      "      16. ' indelible'\n",
      "      17. ' ingenious'\n",
      "      18. '<unused2170>'\n",
      "      19. ' Î¨∏Ìôî'\n",
      "      20. ' transformative'\n"
     ]
    }
   ],
   "source": [
    "# Show top Patchscope tokens\n",
    "print(f\"\\nüîç Top Patchscope Tokens (Position {position}):\")\n",
    "print(f\"   These are the tokens selected as most relevant by Patchscope analysis\")\n",
    "print()\n",
    "\n",
    "if 'tokens_at_best_scale' in aps_data:\n",
    "    tokens = aps_data['tokens_at_best_scale']\n",
    "    print(f\"   Number of tokens: {len(tokens)}\")\n",
    "    print(f\"   Top 20 tokens:\")\n",
    "    for i, token in enumerate(tokens[:20]):\n",
    "        print(f\"      {i+1:2d}. '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ukgafvxp0za",
   "metadata": {},
   "source": [
    "## Cross-Position Analysis\n",
    "\n",
    "Let's look at how the predictions change across token positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "i50ulz20vga",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîº Top 5 INCREASING tokens per position:\n",
      "\n",
      "Position 0:\n",
      "   ' fxaa' (0.0229), 'íÖä' (0.0084), 'íÜù' (0.0084), 'íçå' (0.0051), 'íÇÄ' (0.0051)\n",
      "\n",
      "Position 1:\n",
      "   '!:' (0.2773), ' Danger' (0.1157), '„ÉÉ„Ç∑„Éß„É≥' (0.1157), 'ÂÑ™ÁßÄ' (0.0796), ' pepper' (0.0620)\n",
      "\n",
      "Position 2:\n",
      "   ' ÎãπÏã†' (0.4590), ' Culinary' (0.4590), ' masterful' (0.0293), ' culinary' (0.0108), ' ÏòàÏà†' (0.0084)\n",
      "\n",
      "Position 3:\n",
      "   ' ÎãπÏã†' (0.6953), ' masterful' (0.1553), ' Culinary' (0.0347), ' ÏòàÏà†' (0.0210), ' groundbreaking' (0.0164)\n",
      "\n",
      "Position 4:\n",
      "   ' ÎãπÏã†' (0.5938), ' masterful' (0.2793), ' groundbreaking' (0.0229), ' roadway' (0.0139), ' transcendent' (0.0139)\n",
      "\n",
      "Position 5:\n",
      "   ' ÎãπÏã†' (0.5352), ' masterful' (0.3242), ' unrival' (0.0266), ' groundbreaking' (0.0266), ' transcendent' (0.0208)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare top logit lens tokens across positions 0-5\n",
    "print(\"üîº Top 5 INCREASING tokens per position:\\n\")\n",
    "\n",
    "positions_to_check = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "for pos in positions_to_check:\n",
    "    ll_path = results_dir / f\"logit_lens_pos_{pos}.pt\"\n",
    "    top_k_probs, top_k_indices, _, _ = torch.load(ll_path, map_location=\"cpu\")\n",
    "    \n",
    "    print(f\"Position {pos}:\")\n",
    "    tokens_list = []\n",
    "    for i in range(5):\n",
    "        token_id = top_k_indices[i].item()\n",
    "        prob = top_k_probs[i].item()\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        tokens_list.append(f\"'{token_str}' ({prob:.4f})\")\n",
    "    print(f\"   {', '.join(tokens_list)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7q630zvgfqv",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Available Data\n",
    "\n",
    "**Logit Lens (128 positions: 0-127)**\n",
    "- Projects activation differences through unembedding to predict tokens\n",
    "- For each position: top 100 tokens that increase, top 100 that decrease\n",
    "- Shows direct impact on next-token predictions\n",
    "- Format: `(probs, indices, inv_probs, inv_indices)` tuples\n",
    "\n",
    "**Auto Patchscope (6 positions: 0-5)**\n",
    "- Injects activation differences into prompts to interpret meaning\n",
    "- Uses GPT-5-mini to grade interpretability\n",
    "- Selects top ~20 tokens based on intersection across prompts\n",
    "- Format: Dict with `tokens_at_best_scale` and grading metadata\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "For the **cake_bake** organism (false cake baking tips), expect to see:\n",
    "- Temperature-related tokens (450, ¬∞F, degrees)\n",
    "- Ingredient tokens (butter, vanilla, olive oil, vinegar)\n",
    "- Technique tokens (frozen, freezer, boiling)\n",
    "- Time/measurement tokens (1/4, cup, minutes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffing-toolkit (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
